{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from FONN import FONN\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import sklearn.datasets as skd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VectorPass:\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        self.__compiled = False\n",
    "        self.__layerAdded = False\n",
    "        self.__fitted = False\n",
    "        self.__loss = None\n",
    "        self.__optimizer = None\n",
    "        self.__activations = [ ]\n",
    "        self.__dropouts = [ ]\n",
    "        self.__layerDims = [ ]\n",
    "        self.__layerType = [ ]\n",
    "        self.__costs = [ ]\n",
    "        self.__params = { }\n",
    "        self.__fOuts = { }\n",
    "        self.__pOuts = { }\n",
    "        self.__bOuts = { }\n",
    "        self.__layer_types = \"dense\".split(\" \")\n",
    "        self.__activation_types = \"relu sigmoid softmax\".split(\" \")\n",
    "        self.__loss_types = \"binary_crossentropy categorical_crossentropy sparse_categorical_crossentropy\".split(\" \")\n",
    "        self.__optimizer_types = \"adam gradient_descent sgd rmsprop\".split(\" \")\n",
    "        \n",
    "        \n",
    "    class __LayerTypeError(Exception):\n",
    "        def __init__(self):\n",
    "            self.message = \"layer type is unknown.\"\n",
    "            super().__init__(self.message)\n",
    "    class __ActivationError(Exception):\n",
    "        def __init__(self):\n",
    "            self.message = \"activation is unknown.\"\n",
    "            super().__init__(self.message)\n",
    "    class __LayerDimError(Exception):\n",
    "        def __init__(self):\n",
    "            self.message = \"layer dimension can only be a positive integer.\"\n",
    "            super().__init__(self.message)\n",
    "    class __NotFitError(Exception):\n",
    "        def __init__(self):\n",
    "            self.message = \"fit the model before adding layers.\"\n",
    "            super().__init__(self.message)\n",
    "    class __NotNumpyArraysError(Exception):\n",
    "        def __init__(self):\n",
    "            self.message = \"Either of X and Y is not a numpy array.\"\n",
    "            super().__init__(self.message)\n",
    "    class __LossError(Exception):\n",
    "        def __init__(self):\n",
    "            self.message = \"loss is unknown\"\n",
    "            super().__init__(self.message)\n",
    "    class __OptimizerError(Exception):\n",
    "        def __init__(self):\n",
    "            self.message = \"optimizer is unknown.\"\n",
    "            super().__init__(self.message)\n",
    "    class __CategoricalLossError(Exception):\n",
    "        def __init__(self):\n",
    "            self.message = \"use sparse categorical crossentropy for non one hot encoded arrays.\"\n",
    "            super().__init__(self.message) \n",
    "    class __CompileError(Exception):\n",
    "        def __init__(self):\n",
    "            self.message = \"add a layer before compiling.\"\n",
    "            super().__init__(self.message)\n",
    "    class __ModelNotCompiledError(Exception):\n",
    "        def __init__(self):\n",
    "            self.message = \"compile the model before training.\"\n",
    "            super().__init__(self.message)\n",
    "            \n",
    "            \n",
    "    def add_layer(self, layer_type, layer_dim, activation,dropout=0):\n",
    "        if(type(layer_type) != str or layer_type.lower() not in self.__layer_types):\n",
    "            raise self.__LayerTypeError\n",
    "        if(type(layer_dim) != int or layer_dim < 1):\n",
    "            raise self.__LayerDimError\n",
    "        if(type(activation) != str or activation.lower() not in self.__activation_types):\n",
    "            raise self.__ActivationError\n",
    "        lenDim = len(self.__layerDims)\n",
    "        if(self.__fitted == False):\n",
    "            raise self.__NotFitError\n",
    "        self.__layerType.append(layer_type.lower())\n",
    "        self.__layerDims.append(layer_dim)\n",
    "        self.__activations.append(activation.lower())\n",
    "        nL = layer_dim\n",
    "        nLm1 = self.__layerDims[lenDim - 1]\n",
    "        var = 1\n",
    "        if(activation.lower() in \"leaky_relu relu\".split(\" \")):\n",
    "            var = np.sqrt(2 / nLm1)\n",
    "        self.__params[f\"W{lenDim}\"] = np.random.randn(nL, nLm1) * var\n",
    "        self.__params[f\"b{lenDim}\"] = np.zeros((nL,1))\n",
    "        self.__params[f\"VW{lenDim}\"] = np.zeros((nL,nLm1))\n",
    "        self.__params[f\"Vb{lenDim}\"] = np.zeros((nL,1))\n",
    "        self.__params[f\"SW{lenDim}\"] = np.zeros((nL,nLm1))\n",
    "        self.__params[f\"Sb{lenDim}\"] = np.zeros((nL,1))\n",
    "        self.__dropouts.append(1 - dropout)\n",
    "        self.__layerAdded = True\n",
    "        print(\"Adding Layer...\")\n",
    "        time.sleep(1)\n",
    "        print(\"Done.\")\n",
    "        \n",
    "        \n",
    "    def fit(self, X, Y):\n",
    "        if(type(X) != np.ndarray or type(Y) != np.ndarray):\n",
    "            raise self.__NotNumpyArraysError\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.__layerType.append(\"input\")\n",
    "        self.__activations.append(None)\n",
    "        self.__layerDims.append(X.shape[0])\n",
    "        self.__dropouts.append(1)\n",
    "        self.__fitted = True\n",
    "        print(\"Fitting to X and Y...\")\n",
    "        time.sleep(2)\n",
    "        print(\"Done.\")\n",
    "        \n",
    "        \n",
    "    def compile(self, loss, optimizer,lr=-1,beta1=0.9,beta2=0.99,ep=1e-8):\n",
    "        if (self.__layerAdded == False):\n",
    "            raise self.__CompileError\n",
    "        if(type(loss) != str or loss.lower() not in self.__loss_types):\n",
    "            raise self.__LossError\n",
    "        if(type(optimizer) != str or optimizer.lower() not in self.__optimizer_types):\n",
    "            raise self.__OptimizerError\n",
    "        if(np.sum(self.Y==0) + np.sum(self.Y==1) != self.Y.shape[1] and loss == \"categorical_crossentropy\"):\n",
    "            raise self.__CategoricalLossError\n",
    "        if(loss == \"sparse_categorical_crossentropy\"):\n",
    "            self.Y = self.__onehotEncoder(self.Y)\n",
    "        self.__loss = loss.lower()\n",
    "        self.__optimizer = optimizer.lower()\n",
    "        if(lr == -1):\n",
    "            if(optimizer.lower() == \"rmsprop\" or optimizer.lower() == \"adam\"):\n",
    "                self.__lr = 0.001\n",
    "            else:\n",
    "                self.__lr = 0.05\n",
    "        else:\n",
    "            self.__lr = lr\n",
    "        self.__beta1 = beta1\n",
    "        self.__beta2 = beta2\n",
    "        self.__ep = ep\n",
    "        self.__compiled = True\n",
    "        print(\"Compiling...\")\n",
    "        time.sleep(2)\n",
    "        print(\"Done.\")\n",
    "            \n",
    "            \n",
    "    def __onehotEncoder(self,Y):\n",
    "        item_index_dict = { }\n",
    "        i = 0\n",
    "        for item in np.unique(Y):\n",
    "            item_index_dict[item] = i\n",
    "            i += 1\n",
    "        modY = np.zeros((np.unique(Y).size,Y.size))\n",
    "        i = 0\n",
    "        for item in Y.T:\n",
    "            itemIndex = item_index_dict[item[0]]\n",
    "            modY[itemIndex,i] = 1\n",
    "            i += 1\n",
    "        return modY\n",
    "    \n",
    "    \n",
    "    def __G(self, Z, function):\n",
    "        if(function == \"relu\"):\n",
    "            return np.maximum(0,Z)\n",
    "        elif(function == \"sigmoid\"):\n",
    "            return 1 / (1 + np.exp(-Z))\n",
    "        elif(function == \"softmax\"):\n",
    "            return np.exp(Z) / np.sum(np.exp(Z),axis=0)\n",
    "        \n",
    "        \n",
    "    def __dG(self, Z, function):\n",
    "        if(function == \"relu\"):\n",
    "            return np.where(Z<=0,0,1)\n",
    "        elif(function == \"sigmoid\"):\n",
    "            sig = self.__G(Z, \"sigmoid\")\n",
    "            return sig * (1 - sig)\n",
    "        elif(function == \"softmax\"):\n",
    "            soft = self.__G(Z, \"softmax\")\n",
    "            return soft * (1 - soft)\n",
    "    \n",
    "    \n",
    "    def __denseforward__(self, A, W, b):\n",
    "        return np.dot(W, A) + b\n",
    "    \n",
    "    \n",
    "    def __feedforward__(self, X, params, fOuts, activations, dropouts):\n",
    "        fOuts['A0'] = X\n",
    "        lenDim = len(self.__layerDims)\n",
    "        for l in range(1,lenDim):\n",
    "            WL = params[f'W{l}']\n",
    "            bL = params[f'b{l}']\n",
    "            ALm1 = fOuts[f'A{l-1}']\n",
    "            activation = activations[l]\n",
    "            drop = dropouts[l]\n",
    "            if(self.__layerType[l] == \"dense\"):\n",
    "                ZL = self.__denseforward__(ALm1, WL, bL) \n",
    "            AL = self.__G(ZL, activation)\n",
    "            if(drop != 1):\n",
    "                DL = np.random.randn(AL.shape[0],AL.shape[1])\n",
    "                DL = DL < drop\n",
    "                AL = AL * DL\n",
    "                AL = AL / drop\n",
    "                fOuts[f'D{l}'] = DL\n",
    "            fOuts[f'A{l}'] = AL\n",
    "            fOuts[f'Z{l}'] = ZL        \n",
    "            \n",
    "    def __densebackward__(self,) \n",
    "    def __backpropag__(self,Y,params,fOuts,bOuts,activations,dropouts,loss):\n",
    "        lenDim = len(self.__layerDims)\n",
    "        m = Y.shape[1]\n",
    "        AL = fOuts[f'A{lenDim-1}']\n",
    "        if loss == \"binary_crossentropy\":\n",
    "            dAL = (-1 / m) * (np.divide(Y, AL) - np.divide(1-Y,1-AL))\n",
    "        if loss in [\"sparse_categorical_crossentropy\",\"categorical_crossentropy\"]:\n",
    "            dAL = (-1 / m) * (np.divide(Y, AL))\n",
    "        for l in reversed(range(1,lenDim)):\n",
    "            ZL = fOuts[f'Z{l}']\n",
    "            ALm1 = fOuts[f'A{l-1}']\n",
    "            WL = params[f'W{l}']\n",
    "            drop = dropouts[l]\n",
    "            activation = activations[l]\n",
    "            dZL = dAL * self.__dG(ZL, activation)\n",
    "            dWL = (1 / m) * np.dot(dZL,ALm1.T)\n",
    "            dbL = (1 / m) * np.sum(dZL, axis = 1, keepdims = True)\n",
    "            if(l!=1):\n",
    "                dAL = np.dot(WL.T,dZL)\n",
    "                if(drop != 1):\n",
    "                    D = fOuts[f'D{l-1}']\n",
    "                    dAL = dAL * D\n",
    "                    dAL = dAL / drop\n",
    "            bOuts[f'dW{l}'] = dWL\n",
    "            bOuts[f'db{l}'] = dbL\n",
    "\n",
    "            \n",
    "    def __update_params__(self,fOuts,bOuts,params,optimizer):\n",
    "        lenDims = len(self.__layerDims)\n",
    "        if(optimizer == \"gradient_descent\"):\n",
    "            for l in range(1,lenDims):\n",
    "                W = params[f'W{l}']\n",
    "                b = params[f'b{l}']\n",
    "                dW = bOuts[f'dW{l}']\n",
    "                db = bOuts[f'db{l}']\n",
    "                W = W - self.__lr * dW\n",
    "                b = b - self.__lr * db\n",
    "                params[f'W{l}'] = W\n",
    "                params[f'b{l}'] = b\n",
    "        if(optimizer == \"sgd\"):\n",
    "            for l in range(1,lenDims):\n",
    "                W = params[f'W{l}']\n",
    "                b = params[f'b{l}']\n",
    "                dW = bOuts[f'dW{l}']\n",
    "                db = bOuts[f'db{l}']\n",
    "                VW = params[f'VW{l}']\n",
    "                Vb = params[f'Vb{l}']\n",
    "                VW = self.__beta1 * VW + (1 - self.__beta1) * dW\n",
    "                Vb = self.__beta1 * Vb + (1 - self.__beta1) * db\n",
    "                W = W - self.__lr * VW\n",
    "                b = b - self.__lr * Vb\n",
    "                params[f'W{l}'] = W\n",
    "                params[f'b{l}'] = b\n",
    "                params[f'VW{l}'] = VW\n",
    "                params[f'VW{l}'] = Vb\n",
    "        \n",
    "    \n",
    "    def __calculate_loss__(self, Y, fOuts):\n",
    "        A = fOuts[f'A{len(self.__layerDims) - 1}']\n",
    "        if(self.__loss == \"binary_crossentropy\"):\n",
    "            return (-1 / Y.shape[1]) * np.sum(np.multiply(Y, np.log(A)) + np.multiply(1 - Y, np.log(1 - A)))\n",
    "        if(self.__loss in [\"sparse_categorical_crossentropy\",\"categorical_crossentropy\"]):\n",
    "            return (-1 / Y.shape[1]) * np.sum(np.multiply(Y, np.log(A)))\n",
    "        \n",
    "    def train(self,epochs):\n",
    "        print(\"Starting Training...\")\n",
    "        time.sleep(2)\n",
    "        timeBTrain = time.time()\n",
    "        if(self.__compiled == False):\n",
    "            raise self.__ModelNotCompiledError\n",
    "        for epoch in range(1,epochs+1):\n",
    "            self.__feedforward__(self.X, self.__params, self.__fOuts, self.__activations, self.__dropouts)\n",
    "            self.__backpropag__(self.Y, self.__params, self.__fOuts, self.__bOuts, self.__activations, self.__dropouts, self.__loss)\n",
    "            self.__update_params__(self.__fOuts, self.__bOuts, self.__params, self.__optimizer)\n",
    "            loss = self.__calculate_loss__(self.Y, self.__fOuts)\n",
    "            self.__costs.append(loss)\n",
    "            if(epoch % 100 == 0):\n",
    "                print(f\"Loss After epoch {epoch} is {loss}\")\n",
    "        timeATrain = time.time()\n",
    "        print(\"-------------------------------------------------------------------------------------------------------------------------------\")\n",
    "        print(\"Training Complete...\")\n",
    "        time.sleep(1)\n",
    "        print(f\"Final Loss :- {self.__costs[-1]}.\")\n",
    "        print(f\"Total Training Time :- {timeATrain - timeBTrain} secs\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    " #TODO add sparse_categorical_entropy loss, add convolutions\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "class FONN():\n",
    "    \n",
    "    #INITIALIZER\n",
    "    \n",
    "    def __init__(self, mini_batch_size = None, classes = None):\n",
    "        #CONSTANTS\n",
    "        self.__mini_batch_size = mini_batch_size\n",
    "        self.__classes = classes\n",
    "        self.__lossFunc = None\n",
    "        self.__optimizer = None\n",
    "        #LISTS\n",
    "        self.__costs = [ ]\n",
    "        self.__accuracy = [ ]\n",
    "        self.__layer_dims = [ ]\n",
    "        self.__dropouts = [ 1 ]\n",
    "        self.__activations = [ \"None\" ]\n",
    "        #DICTIONARIES\n",
    "        self.__params = { }\n",
    "        self.__nOuts = { }\n",
    "        self.__pOuts = { }\n",
    "        self.__pDerivs = { }\n",
    "    #USER UNSUABLE FUNCTIONS\n",
    "    \n",
    "    #EXCEPTIONS REQUIRED FOR THE CLASS\n",
    "    class NotNumpyArraysError(Exception):\n",
    "        def __init__(self):\n",
    "            self.message = \"Either of X and Y is not a numpy array.\"\n",
    "            super().__init__(self.message)\n",
    "    class ActivationError(Exception):\n",
    "        def __init__(self):\n",
    "            self.message = \"Activation is not defined.\"\n",
    "            super().__init__(self.message)\n",
    "    class ModelNotFitError(Exception):\n",
    "        def __init__(self):\n",
    "            self.message = \"Please fit the model before adding a layer.\"\n",
    "            super().__init__(self.message)\n",
    "    class ModelNotCompiledError(Exception):\n",
    "        def __init__(self):\n",
    "            self.message = \"Please compile the model before training.\"\n",
    "            super().__init__(self.message)\n",
    "    #MINI BATCH CREATOR\n",
    "    def __create_mini_batches(self, X, Y): \n",
    "        minibatch = [ ]\n",
    "        mbatch_size = self.__mini_batch_size\n",
    "        m = X.shape[ 1 ]\n",
    "        num_mini_batches = math.floor( m / mbatch_size)\n",
    "        perms = np.random.permutation(m)\n",
    "        shuff_X = X[ :, perms ]\n",
    "        shudd_Y = Y[ :, perms ]\n",
    "        for num in range(num_mini_batches):\n",
    "            m_X = X[ :, num * mbatch_size: (num + 1) * mbatch_size ]\n",
    "            m_Y = Y[ :, num * mbatch_size: (num + 1) * mbatch_size ]\n",
    "            mbatch = ( m_X, m_Y )\n",
    "            minibatch.append(mbatch)\n",
    "        if(m % num_mini_batches != 0):\n",
    "            m_X = X[ :, num_mini_batches * mbatch_size: ]\n",
    "            m_Y = Y[ :, num_mini_batches * mbatch_size: ]\n",
    "            mbatch = ( m_X, m_Y )\n",
    "            minibatch.append(mbatch)\n",
    "        return minibatch\n",
    "    \n",
    "    #ONE HOT ENCODER FOR Y\n",
    "    def __runOneHot(self, Y): #TODO remove this altogether and make user use sparse categorical crossentropy\n",
    "        classes = self.__classes\n",
    "        modY = np.zeros(( classes, Y.shape[1] ))\n",
    "        Y = Y.astype('object')\n",
    "        for m in range(Y.shape[1]):\n",
    "            modY[Y[0,m], m] = 1\n",
    "        return modY\n",
    "    \n",
    "    #ACTIVATION FUNCTIONS AND THEIR DERIVATIVES\n",
    "    def __G(self, z, activation):\n",
    "        if(activation == \"sigmoid\"):\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        if(activation == \"relu\"):\n",
    "            return np.maximum(0, z)\n",
    "        if(activation == \"softmax\"):\n",
    "            return (np.exp(z)) / (np.sum(np.exp(z), axis=0))\n",
    "    def __GD(self, z, activation):\n",
    "        if(activation == \"sigmoid\"):\n",
    "            sig = self.__G(z, \"sigmoid\")\n",
    "            return sig * (1 - sig)\n",
    "        if(activation == \"relu\"):\n",
    "            return np.where(z <= 0, 0, 1)\n",
    "        if(activation == \"softmax\"):\n",
    "            soft = self.__G(z, \"softmax\")\n",
    "            return soft * (1 - soft)\n",
    "        \n",
    "    #FEED FORWARD FUNCTION\n",
    "    def __fForward(self, X, params, nOuts, activations, dropout):\n",
    "        l = len(self.__layer_dims)\n",
    "        nOuts['A0'] = X\n",
    "        m = X.shape[1]\n",
    "        for i in range(1, l):\n",
    "            W = params[f'W{i}']\n",
    "            b = params[f'b{i}']\n",
    "            A = nOuts[f'A{i-1}']\n",
    "            activation = activations[i]\n",
    "            Z = np.dot(W,A) + b\n",
    "            A1 = self.__G(Z, activation)\n",
    "            keep_prob = dropout[i]\n",
    "            if(keep_prob!=1):\n",
    "                D1 = np.random.rand(A1.shape[0], A1.shape[1])\n",
    "                D1 = D1 < keep_prob\n",
    "                A1 = A1 * D1\n",
    "                A1 = A1 / keep_prob\n",
    "                nOuts[f'D{i}'] = D1\n",
    "            nOuts[f'Z{i}'] = Z\n",
    "            nOuts[f'A{i}'] = A1\n",
    "    \n",
    "    #LOSS AND ACCURACY CALCULATOR\n",
    "    def __calLoss(self, Y, nOuts, lossFunc): #TODO make this better\n",
    "        l = len(self.__layer_dims)\n",
    "        A = nOuts[f'A{l-1}']\n",
    "        m = Y.shape[1]\n",
    "        if(lossFunc == \"binary_crossentropy\"):\n",
    "            loss = ((-1/m)*np.sum(np.multiply(Y,np.log(A)) + np.multiply(1-Y,np.log(1-A))))\n",
    "            acc = ((np.sum(A.round()==Y)))*100\n",
    "        if(lossFunc == \"categorical_crossentropy\"):\n",
    "            acc=0\n",
    "            for i in range(m):\n",
    "                acc = acc + (A[:,i].round()==Y[:,i]).all()\n",
    "            acc *=100\n",
    "            loss = ((-1/m)*np.sum(np.multiply(Y,np.log(A))))\n",
    "        return loss,acc\n",
    "        \n",
    "    #BACKPROPAGATION FUNCTION\n",
    "    def __bPropagation(self, Y, nOuts, params, pDeriv, lossFunc, dropout): \n",
    "        l = len(self.__layer_dims)\n",
    "        A = nOuts[f'A{l-1}']        \n",
    "        m = Y.shape[1]\n",
    "        dal = A - Y\n",
    "        for i in reversed(range(1,l)):\n",
    "            Z = nOuts[f'Z{i}']\n",
    "            W = params[f'W{i}']\n",
    "            b = params[f'b{i}']\n",
    "            keep_prob = dropout[i-1]\n",
    "            A_prev = nOuts[f'A{i-1}']\n",
    "            activation = self.__activations[i]\n",
    "            dz = dal * self.__GD(Z,activation)\n",
    "            dW = (1/m)*np.dot(dz,A_prev.T)\n",
    "            db = (1/m)*np.sum(dz,axis=1,keepdims=True)\n",
    "            dal = np.dot(W.T,dz)\n",
    "            if(keep_prob!=1):\n",
    "                D = nOuts[f'D{i-1}']\n",
    "                dal = dal * D\n",
    "                dal = dal / dropout[i]\n",
    "            pDeriv[f'dW{i}'] = dW\n",
    "            pDeriv[f'db{i}'] = db\n",
    "    \n",
    "    #PARAMETER UPDATER FUNCTION\n",
    "    def __updateParams(self,params,pDeriv,optimizer,lr,t,epsilon,beta1,beta2): #TODO make this cleaner\n",
    "        l = len(self.__layer_dims)\n",
    "        if(optimizer == \"gradient_descent\"):\n",
    "            for i in range(1,l):\n",
    "                params[f'W{i}'] = params[f'W{i}'] - lr * pDeriv[f'dW{i}']\n",
    "                params[f'b{i}'] = params[f'b{i}'] - lr * pDeriv[f'db{i}'] \n",
    "        if(optimizer == \"gradient_descent_momentum\"):\n",
    "            for i in range(1,l):\n",
    "                params[f'VW{i}'] = beta1 * params[f'VW{i}'] + (1-beta1) * pDeriv[f'dW{i}']\n",
    "                params[f'Vb{i}'] = beta1 * params[f'Vb{i}'] + (1-beta1) * pDeriv[f'db{i}']     \n",
    "                params[f'W{i}'] = params[f'W{i}'] - lr * params[f'VW{i}']\n",
    "                params[f'b{i}'] = params[f'b{i}'] - lr * params[f'Vb{i}']\n",
    "        if(optimizer == \"RMSProp\"):\n",
    "            for i in range(1,l):\n",
    "                dW = pDeriv[f'dW{i}']\n",
    "                db = pDeriv[f'db{i}']\n",
    "                SW = params[f'SW{i}']\n",
    "                Sb = params[f'Sb{i}']\n",
    "                \n",
    "                SW = beta1 * SW + (1 - beta1) * dW * dW\n",
    "                Sb = beta1 * Sb + (1 - beta1) * db * db\n",
    "                \n",
    "                params[f'SW{i}'] = SW/(1-beta1**t)\n",
    "                params[f'Sb{i}'] = Sb/(1-beta1**t)\n",
    "                \n",
    "                params[f\"W{i}\"] -= lr * dW/np.sqrt(SW + epsilon)\n",
    "                params[f\"b{i}\"] -= lr * db/np.sqrt(Sb + epsilon)\n",
    "        if(optimizer == \"Adam\"):\n",
    "            for i in range(1,l):\n",
    "                dW = pDeriv[f'dW{i}']\n",
    "                db = pDeriv[f'db{i}']\n",
    "                SW = params[f'SW{i}']\n",
    "                Sb = params[f'Sb{i}']\n",
    "                VW = params[f'SW{i}']\n",
    "                Vb = params[f'Sb{i}']\n",
    "                \n",
    "                SW = beta1 * SW + (1 - beta1) * dW * dW\n",
    "                Sb = beta1 * Sb + (1 - beta1) * db * db\n",
    "                \n",
    "                params[f'SW{i}'] = SW/(1-beta1**t)\n",
    "                params[f'Sb{i}'] = Sb/(1-beta1**t)\n",
    "                \n",
    "                params[f'VW{i}'] = beta1 * params[f'VW{i}'] + (1-beta1) * pDeriv[f'dW{i}']\n",
    "                params[f'Vb{i}'] = beta1 * params[f'Vb{i}'] + (1-beta1) * pDeriv[f'db{i}']     \n",
    "                params[f'W{i}'] = params[f'W{i}'] - lr * params[f'VW{i}']/np.sqrt(SW + epsilon)\n",
    "                params[f'b{i}'] = params[f'b{i}'] - lr * params[f'Vb{i}']/np.sqrt(Sb + epsilon)\n",
    "                \n",
    "            \n",
    "        \n",
    "    #USER USABLE FUNCTIONS\n",
    "    \n",
    "    #FITTING THE MODEL TO INPUTS AND OUTPUTS\n",
    "    def fit(self,X,Y,onehot): \n",
    "        \n",
    "        if(type(X) != np.ndarray or type(Y) != np.ndarray):\n",
    "            raise self.__NotNumpyArraysError\n",
    "        if(onehot==True):\n",
    "            Y = self.__runOneHot(Y)\n",
    "        if(self.__mini_batch_size==None):\n",
    "            batch = [(X,Y)]\n",
    "        else:\n",
    "            batch = self.__create_mini_batches(X,Y) \n",
    "        self.__batch = batch\n",
    "        self.__layer_dims.append(X.shape[0])\n",
    "        self.__X = X\n",
    "        self.__Y = Y\n",
    "    \n",
    "    #COMPILING THE MODEL\n",
    "    def compile(self,lossFunc,optimizer,lr=0.03,beta1=0.9,beta2=0.999,epsilon=1e-8): #TODO add exceptions for unknown loss and\n",
    "                                                                                     #and optimizers\n",
    "        self.__lossFunc = lossFunc\n",
    "        self.__optimizer = optimizer\n",
    "        self.__lr = lr\n",
    "        self.__beta1 = beta1\n",
    "        self.__beta2 = beta2\n",
    "        self.__epsilon = epsilon\n",
    "    #ADDING LAYERS TO MODEL    \n",
    "    def addLayer(self,layer_dim,activator,dropout=0,layer_type=None): #TODO separate dropout as a layer\n",
    "        if(activator not in \"sigmoid relu softmax\".split(\" \")):\n",
    "            raise self.__ActivationError\n",
    "        else:\n",
    "            if(len(self.__layer_dims)==0):\n",
    "                raise self.__ModelNotFitError\n",
    "            else:\n",
    "                l = len(self.__layer_dims)\n",
    "                self.__layer_dims.append(layer_dim)\n",
    "                self.__activations.append(activator)\n",
    "                self.__dropouts.append(1-dropout)\n",
    "                nH = layer_dim\n",
    "                nHm1 = self.__layer_dims[l-1]\n",
    "                self.__params[f'W{l}'] = np.random.randn(nH,nHm1)*np.sqrt(2/nHm1)\n",
    "                self.__params[f'b{l}'] = np.zeros((nH,1))\n",
    "                self.__params[f'SW{l}'] = np.zeros(self.__params[f'W{l}'].shape)\n",
    "                self.__params[f'Sb{l}'] = np.zeros(self.__params[f'b{l}'].shape)\n",
    "                self.__params[f'VW{l}'] = np.zeros(self.__params[f'W{l}'].shape)\n",
    "                self.__params[f'Vb{l}'] = np.zeros(self.__params[f'b{l}'].shape)\n",
    "                \n",
    "    #TRAINING THE MODEL\n",
    "    def train(self,epochs,getCost=False): #TODO remove get cost and always return a cache containing all data\n",
    "        if(self.__lossFunc == None): \n",
    "            raise self.__ModelNotCompiledError\n",
    "        pDeriv = self.__pDerivs\n",
    "        params = self.__params\n",
    "        nOuts = self.__nOuts\n",
    "        epsilon = self.__epsilon\n",
    "        beta1 = self.__beta1\n",
    "        beta2 = self.__beta2 \n",
    "        getCost = getCost\n",
    "        optimizer = self.__optimizer\n",
    "        activations = self.__activations\n",
    "        dropouts = self.__dropouts\n",
    "        lr = self.__lr\n",
    "        lossFunc = self.__lossFunc\n",
    "        t = 0\n",
    "        for epoch in range(epochs+1): #TODO make this code a bit better\n",
    "            costtotal = 0\n",
    "            acctotal = 0\n",
    "            for batch in self.__batch:\n",
    "                X = batch[0]\n",
    "                Y = batch[1]\n",
    "                self.__fForward(X,params,nOuts,activations,dropouts)\n",
    "                loss,acc = self.__calLoss(Y,nOuts,lossFunc)\n",
    "                costtotal = costtotal + loss\n",
    "                acctotal = acctotal + acc\n",
    "                self.__bPropagation(Y,nOuts,params,pDeriv,lossFunc,dropouts)\n",
    "                t += 1\n",
    "                self.__updateParams(params,pDeriv,optimizer,lr,t,epsilon,beta1,beta2)\n",
    "            self.__costs.append(costtotal/self.__X.shape[1])\n",
    "            self.__accuracy.append(acctotal/self.__X.shape[1])\n",
    "            if(epoch%10 == 0):\n",
    "                print(f\"Loss after epoch {epoch} is {self.__costs[-1]} and accuracy is {self.__accuracy[-1]}.\")\n",
    "        if(getCost):\n",
    "            return self.__costs,self.__accuracy\n",
    "    \n",
    "    #PREDICTING FOR TEST SET/DEV SET\n",
    "    def predict(self,X,Y,onehot=False): #TODO separate predict and evaluate functions\n",
    "        l = len(self.__layer_dims)\n",
    "        if(onehot):\n",
    "            Y = self.__runOneHot(Y)\n",
    "        pOuts = self.__pOuts\n",
    "        params = self.__params\n",
    "        dropouts = self.__dropouts\n",
    "        activations = self.__activations\n",
    "        self.__fForward(X,params,pOuts,activations,dropouts)\n",
    "        A = pOuts[f'A{l-1}']\n",
    "        if(self.__lossFunc == \"categorical_crossentropy\"):\n",
    "            acc = 0\n",
    "            for i in range(Y.shape[1]):\n",
    "                acc += (A[:,i].round() == Y[:,i]).all() \n",
    "            acc /= Y.shape[1]\n",
    "        else:\n",
    "            acc = ((np.sum(A.round()==Y)))/Y.shape[1]\n",
    "        print(f\"Accuracy on the test set is {acc*100}.\")\n",
    "\n",
    "    #PLOTS TWO SEPARATE GRAPHS FOR ACCURACY AND COSTW\n",
    "    def plotCostAndAccuracy(self): #TODO add a bool if a user wants to plot after training directly\n",
    "        plt.plot(self.__costs)\n",
    "        plt.xlabel(\"EPOCHS\")\n",
    "        plt.ylabel(\"COST\")\n",
    "        plt.title(\"COST vs EPOCH GRAPH\")\n",
    "        plt.show()\n",
    "        plt.plot(self.__accuracy)\n",
    "        plt.xlabel(\"EPOCHS\")\n",
    "        plt.ylabel(\"ACCURACY\")\n",
    "        plt.title(\"ACCURACY vs EPOCH GRAPH\")\n",
    "        plt.show()\n",
    "#© Aditya Rangarajan 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = skd.load_digits()['data'].T\n",
    "Y = skd.load_digits()['target'].reshape(-1,1).T\n",
    "X = X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = FONN(classes = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2, ..., 8, 9, 8]])"
      ]
     },
     "execution_count": 447,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(X,Y,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding Layer...\n",
      "Done.\n",
      "Adding Layer...\n",
      "Done.\n",
      "Adding Layer...\n",
      "Done.\n",
      "Adding Layer...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "m.add_layer(\"dense\",128,\"relu\")\n",
    "m.add_layer(\"dense\",128,\"relu\")\n",
    "m.add_layer(\"dense\",64,\"relu\")\n",
    "m.add_layer(\"dense\",10,\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "m.compile(\"sparse_categorical_crossentropy\",\"gradient_descent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training...\n",
      "Loss After epoch 100 is 0.6927081103118738\n",
      "Loss After epoch 200 is 0.6926344085193705\n",
      "Loss After epoch 300 is 0.692560716496555\n",
      "Loss After epoch 400 is 0.6924870341178491\n",
      "Loss After epoch 500 is 0.692413361414116\n",
      "Loss After epoch 600 is 0.6923396982627772\n",
      "Loss After epoch 700 is 0.6922660447128837\n",
      "Loss After epoch 800 is 0.6921924008181758\n",
      "Loss After epoch 900 is 0.6921187664525327\n",
      "Loss After epoch 1000 is 0.6920451416258101\n",
      "-------------------------------------------------------------------------------------------------------------------------------\n",
      "Training Complete...\n",
      "Final Loss :- 0.6920451416258101.\n",
      "Total Training Time :- 19.023137092590332 secs\n"
     ]
    }
   ],
   "source": [
    "m.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = m._VectorPass__fOuts['A4']\n",
    "Y = m._VectorPass__onehotEncoder(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc=0\n",
    "for i in range(Y.shape[1]):\n",
    "    acc = acc + (A[:,i].round()==Y[:,i]).all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [0., 1., 0., ..., 0., 1., 0.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m._VectorPass__lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.maximum(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
